import time
from llama_index.core import VectorStoreIndex, Settings, PromptTemplate
from llama_index.vector_stores.chroma import ChromaVectorStore
import chromadb
from llama_index.llms.ollama import Ollama
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

# --- Configuration RAG ---
CHROMA_COLLECTION_NAME = "burkina_knowledge_base"
EMBED_MODEL = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
LLM_MODEL = "mistral:7b-instruct-v0.2-q4_K_M"
REQUEST_TIMEOUT = 360.0

# ‚úÖ PROMPT PERSONNALIS√â EN FRAN√áAIS
QA_PROMPT_TEMPLATE = """Tu es un assistant expert sur le Burkina Faso. R√©ponds TOUJOURS en fran√ßais.

Contexte fourni :
{context_str}

Question : {query_str}

Instructions :
- R√©ponds UNIQUEMENT en fran√ßais
- Base ta r√©ponse UNIQUEMENT sur le contexte fourni ci-dessus
- Si l'information n'est pas dans le contexte, dis "Je ne trouve pas cette information dans les documents fournis"
- Cite les sources quand c'est pertinent
- Sois pr√©cis et concis

R√©ponse en fran√ßais :"""

# 1. Connexion au LLM et aux Embeddings
print(f"Chargement du mod√®le : {LLM_MODEL}")
try:
    llm = Ollama(
        model=LLM_MODEL,
        base_url="http://localhost:11434",
        request_timeout=REQUEST_TIMEOUT
    )

    # Test rapide
    test = llm.complete("R√©ponds en fran√ßais : Bonjour")
    print(f"‚úÖ LLM actif (test: {test.text[:50]}...)")

    Settings.llm = llm
    Settings.embed_model = HuggingFaceEmbedding(model_name=EMBED_MODEL)

except Exception as e:
    print(f"‚ùå ERREUR : {e}")
    exit()

# 2. R√©cup√©ration de l'Index
print("\nConnexion √† l'Index...")
db = chromadb.PersistentClient(path="./chroma_db")
try:
    chroma_collection = db.get_collection(CHROMA_COLLECTION_NAME)
    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
    index = VectorStoreIndex.from_vector_store(vector_store=vector_store)
    print("‚úÖ Index charg√©")
except Exception as e:
    print(f"‚ùå ERREUR : {e}")
    exit()

# 3. Cr√©ation du Retriever
retriever = index.as_retriever(similarity_top_k=8)

# 4. La Requ√™te
print("\n" + "=" * 50)
question = input("Posez votre question : ")
print("=" * 50)

print("\n[PHASE 1 : R√©cup√©ration] en cours...")
retrieved_nodes = retriever.retrieve(question)

print(f"\nüìö {len(retrieved_nodes)} fragments r√©cup√©r√©s :")
if not retrieved_nodes:
    print("‚ùå Aucune source trouv√©e")
    exit()

for i, node in enumerate(retrieved_nodes):
    print(f"\n  üìÑ Source {i + 1} : {node.metadata.get('source', 'Inconnue')}")
    print(f"     Score : {node.score:.4f}")
    print(f"     Extrait : {node.text[:200]}...")

# ‚úÖ 5. Cr√©ation du Query Engine AVEC PROMPT PERSONNALIS√â
qa_prompt = PromptTemplate(QA_PROMPT_TEMPLATE)

query_engine = index.as_query_engine(
    text_qa_template=qa_prompt,
    similarity_top_k=8
)

print(f"\n[PHASE 2 : G√©n√©ration avec {LLM_MODEL}]...")
print("‚è≥ Patientez 30-60 secondes...")

start_time = time.time()

try:
    response = query_engine.query(question)
except Exception as e:
    print(f"\n‚ùå ERREUR : {e}")
    exit()

end_time = time.time()

# 6. Affichage
print("\n" + "=" * 50)
print(f"‚è±Ô∏è  Temps : {end_time - start_time:.2f}s")
print("=" * 50)

print("\nü§ñ R√©ponse :")
print("-" * 50)
if response.response:
    print(response.response)
else:
    print("‚ùå R√©ponse vide")
print("-" * 50)

# ‚úÖ BONUS : Afficher les m√©tadonn√©es des sources utilis√©es
if hasattr(response, 'source_nodes') and response.source_nodes:
    print("\nüìö Sources utilis√©es pour cette r√©ponse :")
    for i, node in enumerate(response.source_nodes[:3]):  # Top 3
        print(f"  {i + 1}. {node.metadata.get('source', 'Inconnue')} (score: {node.score:.3f})")
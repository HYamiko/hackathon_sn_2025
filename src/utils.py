import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import os
from pathlib import Path
import time


def telecharger_pdfs(url_page, dossier_destination="pdfs_telecharges"):
    """
    T√©l√©charge tous les PDFs trouv√©s sur une page web.

    Args:
        url_page: L'URL de la page √† scanner
        dossier_destination: Le dossier o√π sauvegarder les PDFs
    """

    # Cr√©er le dossier de destination s'il n'existe pas
    Path(dossier_destination).mkdir(parents=True, exist_ok=True)

    print(f"üîç Analyse de la page : {url_page}\n")

    try:
        # R√©cup√©rer la page web
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(url_page, headers=headers, timeout=15)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, 'html.parser')

        # Trouver tous les liens PDF
        liens_pdf = []

        # M√©thode 1 : Liens <a> pointant vers des .pdf
        for lien in soup.find_all('a', href=True):
            href = lien['href']
            if href.lower().endswith('.pdf'):
                url_complete = urljoin(url_page, href)
                liens_pdf.append(url_complete)

        # M√©thode 2 : Balises <embed> ou <object> avec des PDFs
        for tag in soup.find_all(['embed', 'object'], src=True):
            src = tag.get('src') or tag.get('data')
            if src and src.lower().endswith('.pdf'):
                url_complete = urljoin(url_page, src)
                liens_pdf.append(url_complete)

        # M√©thode 3 : iframes contenant des PDFs
        for iframe in soup.find_all('iframe', src=True):
            src = iframe['src']
            if '.pdf' in src.lower():
                url_complete = urljoin(url_page, src)
                liens_pdf.append(url_complete)

        # Supprimer les doublons
        liens_pdf = list(set(liens_pdf))

        if not liens_pdf:
            print("‚ùå Aucun PDF trouv√© sur cette page.")
            return

        print(f"‚úÖ {len(liens_pdf)} PDF(s) trouv√©(s)\n")

        # T√©l√©charger chaque PDF
        for i, url_pdf in enumerate(liens_pdf, 1):
            try:
                print(f"üì• [{i}/{len(liens_pdf)}] T√©l√©chargement : {url_pdf}")

                # R√©cup√©rer le PDF
                response_pdf = requests.get(url_pdf, headers=headers, timeout=30, stream=True)
                response_pdf.raise_for_status()

                # V√©rifier que c'est bien un PDF
                content_type = response_pdf.headers.get('Content-Type', '')
                if 'pdf' not in content_type.lower():
                    print(f"   ‚ö†Ô∏è  Avertissement : Le fichier n'est peut-√™tre pas un PDF (type: {content_type})")

                # G√©n√©rer un nom de fichier
                nom_fichier = extraire_nom_fichier(url_pdf, i)
                chemin_complet = os.path.join(dossier_destination, nom_fichier)

                # Sauvegarder le fichier
                with open(chemin_complet, 'wb') as f:
                    for chunk in response_pdf.iter_content(chunk_size=8192):
                        f.write(chunk)

                taille = os.path.getsize(chemin_complet) / (1024 * 1024)  # En Mo
                print(f"   ‚úì Sauvegard√© : {nom_fichier} ({taille:.2f} Mo)\n")

                # Pause pour √©viter de surcharger le serveur
                time.sleep(1)

            except requests.exceptions.RequestException as e:
                print(f"   ‚ùå Erreur lors du t√©l√©chargement : {e}\n")
                continue
            except Exception as e:
                print(f"   ‚ùå Erreur inattendue : {e}\n")
                continue

        print(f"üéâ T√©l√©chargement termin√© ! Fichiers dans : {dossier_destination}/")

    except requests.exceptions.RequestException as e:
        print(f"‚ùå Erreur lors de l'acc√®s √† la page : {e}")
    except Exception as e:
        print(f"‚ùå Erreur inattendue : {e}")


def extraire_nom_fichier(url, numero):
    """Extrait un nom de fichier propre depuis l'URL."""
    # Essayer d'obtenir le nom depuis l'URL
    parsed = urlparse(url)
    nom = os.path.basename(parsed.path)

    # Si pas de nom ou nom invalide, g√©n√©rer un nom
    if not nom or not nom.endswith('.pdf'):
        nom = f"document_{numero}.pdf"

    # Nettoyer le nom (enlever les caract√®res sp√©ciaux)
    nom = nom.replace('%20', '_').replace(' ', '_')

    return nom


def telecharger_pdfs_recursif(url_page, dossier_destination="pdfs_telecharges", profondeur_max=1):
    """
    Version avanc√©e : explore aussi les liens de la page pour trouver plus de PDFs.

    Args:
        url_page: L'URL de d√©part
        dossier_destination: Dossier de sauvegarde
        profondeur_max: Nombre de niveaux de liens √† explorer (0 = page actuelle uniquement)
    """

    urls_visitees = set()
    pdfs_trouves = set()

    def explorer(url, profondeur):
        if profondeur > profondeur_max or url in urls_visitees:
            return

        urls_visitees.add(url)
        print(f"üîç Exploration (niveau {profondeur}): {url}")

        try:
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
            response = requests.get(url, headers=headers, timeout=15)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, 'html.parser')

            # Chercher les PDFs
            for lien in soup.find_all('a', href=True):
                href = lien['href']
                url_complete = urljoin(url, href)

                if url_complete.lower().endswith('.pdf'):
                    pdfs_trouves.add(url_complete)
                elif profondeur < profondeur_max and urlparse(url_complete).netloc == urlparse(url_page).netloc:
                    # Explorer les liens du m√™me domaine
                    explorer(url_complete, profondeur + 1)

            time.sleep(0.5)

        except Exception as e:
            print(f"   Erreur : {e}")

    # Commencer l'exploration
    explorer(url_page, 0)

    print(f"\n‚úÖ {len(pdfs_trouves)} PDF(s) unique(s) trouv√©(s)\n")

    # T√©l√©charger tous les PDFs trouv√©s
    # [Code de t√©l√©chargement similaire √† telecharger_pdfs]


# ===== UTILISATION =====
if __name__ == "__main__":
    # Exemple simple
    url = "https://www.agriculture.bf/document-library/"
    telecharger_pdfs(url)

    # Exemple avec dossier personnalis√©
    # telecharger_pdfs(url, dossier_destination="mes_documents_pdf")

    # Exemple avec exploration r√©cursive (ATTENTION : peut t√©l√©charger beaucoup de fichiers)
    # telecharger_pdfs_recursif(url, profondeur_max=2)